### **赛题背景介绍：BirdCLEF+2025**

#### **1. ****赛事目标**

BirdCLEF+
2025 是一项专注于利用音频数据识别物种的机器学习竞赛，重点关注哥伦比亚马格达莱纳河中游山谷（Middle Magdalena Valley）的El Silencio自然保护区中的鸟类、两栖动物、哺乳动物和昆虫。通过开发先进的计算方法，参赛者需要处理连续的音频数据，并根据声音特征识别不同分类群的物种。

核心目标包括是在复杂的声音景观（soundscape）中准确识别多个物种

知识点:

1.单类识别：专注于识别一种类型或类别的存在与否。

2.多类识别：从多个类别中选择一个作为正确的分类结果。

3.多种类识别（多标签识别）：允许同一对象被分配给多个类别或标签

#### **2. ****评价指标**** ROC-AUC**

1.真阳性率 (TPR)：也称为灵敏度（Sensitivity）或召回率（Recall），表示实际为正类的样本中被正确识别为正类的比例。

2.假阳性率 (FPR)：表示实际为负类的样本中被错误地识别为正类的比例

ROC-AUC是两个相关概念的组合：ROC（接收者操作特性曲线，Receiver Operating Characteristic Curve）和AUC（曲线下面积，Area Under the Curve）。这两个概念共同用于评估分类模型的性能，特别是在二分类问题中。

ROC曲线是一种图形化表示，它展示了不同决策阈值下的真阳性率（True Positive Rate, TPR，也称为灵敏度或召回率）相对于假阳性率（False
Positive Rate, FPR）的变化。通过改变模型输出为正类的概率阈值（默认通常是0.5），可以生成一系列点来绘制这条曲线。

AUC是指ROC曲线下的面积。它提供了一种量化的方法来衡量模型区分正负样本的能力。AUC的取值范围是从0到1，其中：

AUC = 1: 表示一个完美的分类器。

AUC = 0.5: 表示一个随机猜测的分类器（没有区分能力）。

AUC < 0.5: 比随机猜测还要差，这通常意味着预测结果可能是反向的。

**3. ****数据解析**

数据查看地址：[https://www.kaggle.com/competitions/birdclef-2025/data](https://www.kaggle.com/competitions/birdclef-2025/data)

训练音频数据（train_audio/）：包含由xeno-canto.org、iNaturalist和哥伦比亚声音档案库（CSA）提供的短音频片段，每个片段代表一个特定物种的声音。音频已统一重采样至32kHz，并以ogg格式存储。

测试音频数据（test_soundscapes/）：提交时可用，约700段1分钟长的ogg音频，模拟真实环境下的连续录音。

无标签音频数据（train_soundscapes/）：来自与测试数据相同地点的未标注录音，可用于半监督学习。

元数据文件（train.csv）：包含训练音频的详细信息，如物种代码、次级标签、地理位置、质量评分等。

物种信息（taxonomy.csv）：列出所有涉及物种的分类信息，包括iNaturalist分类ID和类别（鸟纲、两栖纲、哺乳纲、昆虫纲）。

6.地理信息文件（recording_location.txt）：提供El Silencio自然保护区的基本位置信息。

**4. ****代码解析**

**4.1 ****baseline****解析（基础方案）**

Baseline地址：

[https://www.kaggle.com/code/kumarandatascientist/bird25-weightedblend-0-88](https://www.kaggle.com/code/kumarandatascientist/bird25-weightedblend-0-88)（请切换到version2）

Baseline本地文件：bird25-weightedblend-0-88.ipynb(与上方同一代码)

**Baseline** **为集成模型，集成模型一权重0.25，** **模型一** **代码流程概述** **：**

### 一、参数设置与输出处理函数

#### 1.

apply_power_to_low_ranked_cols 函数

该函数对预测结果中排名靠后的物种（低置信度）进行幂次放大（如指数为2），以提升模型对稀有物种的敏感度。

#### 2. 文件路径与调试模式

默认读取测试集目录下的ogg文件名，若无测试数据（如本地运行），则进入 debug 模式，使用训练集中的 unlabeled 数据代替

#### 3. 音频参数设定

每个片段长度：5秒

采样率：32kHz

使用 Mel Spectrogram 提取特征：

n_fft = 1024

hop_length = 512

mel bins = 128

f_min = 40Hz, f_max =
16000Hz

快速傅里叶变换（FFT）：对每一帧数据进行快速傅里叶变换，将其从时域转换到频域。这样可以获得该帧内包含的所有频率成分及其强度。

#### 4. 特征归一化

对 Mel 谱图进行标准化处理（均值为0，标准差为1）

**二. ****模型定义与加载**

#### 1. 模型结构：TimmSED

基于 timm 库构建的 SED（Sound Event Detection）模型

使用 ECA-NFNet-L0 作为 backbone

引入注意力机制模块 AttBlockV2

模型输出为 logits，并通过 sigmoid 转换为概率

#### 2. 权重初始化函数

包括卷积层、BN层、全连接层等的权重初始化策略，确保模型训练更稳定

#### 3. 模型加载

加载三个预训练模型（sed0.pth、sed1.pth、sed2.pth）

所有模型都设置为评估模式（.eval()）

 **三．推理过程（** **prediction ****函数）**

#### 1. 音频加载与特征提取

使用 audio_to_mel 函数将音频转为 12 个 5 秒片段的 Mel 谱图

每个片段经过模型推理后得到一个预测向量（对应所有物种的概率）

#### 2. 多模型集成 + 后处理

对三个模型的结果取平均

对低排名物种应用幂次增强（apply_power_to_low_ranked_cols）

#### 3. 结果保存格式

每个5秒片段生成一个 row_id（如 soundscape_12345_20）

每个物种作为一个列，保存其预测概率

**四．后处理优化（时间平滑）**

对同一段录音的12个片段进行时序平滑处理：

中间片段使用前后加权平均（0.2, 0.6,
0.2）

首尾片段只用相邻片段辅助修正（0.8, 0.2）

目的：缓解模型对短时变化的过度敏感，提升鲁棒性。

模型一训练来源：

[https://www.kaggle.com/code/myso1987/birdclef2025-2-train-baseline-5s](https://www.kaggle.com/code/myso1987/birdclef2025-2-train-baseline-5s)

(不是重点，不做介绍)

 **集成模型二权重0.75，** **模型二** **代码模块详解** **：**

### 1. ``<span lang="EN-US">`load_sample(path,

cfg)`：音频切片与预处理

#### 功能：

#### 从 .ogg 或 .wav 文件中读取音频，并将其切分为多个片段用于模型推理。

#### 输入参数：

path: 音频文件路径

cfg: 配置对象（包含采样率、目标时长等）

#### 处理流程：

使用 soundfile 读取音频数据

根据目标时长划分时间点 seconds

将原始音频复制三份进行拼接，防止越界

对每个时间点裁剪音频并添加前后 padding

边缘片段只在一边补零以避免无效区域

#### 输出：

返回所有裁剪好的音频片段列表，用于后续转成梅尔频谱图并输入模型推理

---

`<span lang="EN-US">2. sigmoid(x)</span>``<span>：激活函数<span lang="EN-US"><o:p></o:p></span></span>`

#### 功能：

用于将模型输出的 logit 值转换为概率值 [0, 1] 范围内。

#### 应用场景：

在推理阶段将模型输出经过 sigmoid 变换得到最终预测分数

---

`<span lang="EN-US">3. find_model_files(cfg)</span>``<span>：查找模型文件<span lang="EN-US"><o:p></o:p></span></span>`

#### 功能：

递归查找指定目录下的所有
.pth 模型文件，用于集成推理。

---

### `<span lang="EN-US">4. load_models(cfg, num_classes)</span>``<span>：加载模型权重<span lang="EN-US"><o:p></o:p></span></span>`

#### 功能：

从配置文件中加载多个模型权重，构建集成模型组。

#### 流程说明：

加载每个 .pth 文件

构建对应的 BirdCLEFModel

设置为 eval 模式并移动到相应设备（CPU）

添加到模型列表

#### 注意事项：

支持多模型集成推理

如果没有找到模型文件则报错提示

---

``<span lang="EN-US">`5. predict_on_spectrogram(audio_path, models, cfg,

species_ids)```<span>`：单个音频推理`<span lang="EN-US">`<o:p></o:p>`

#### 功能：

对一个音频文件进行推理，返回每
5 秒片段的预测结果。

#### 流程说明：

调用 load_sample() 获取音频片段

对每个片段：

构造输入张量

使用模型进行推理

多模型取平均

返回 row_id 和预测结果

---

`<span lang="EN-US">6. run_inference(cfg, models, species_ids)</span>``<span>：批量推理<span lang="EN-US"><o:p></o:p></span></span>`

#### 功能：

使用线程池并发地对所有测试音频文件进行推理。

#### 技术亮点：

使用 ThreadPoolExecutor 并发执行

支持多文件同时推理

返回所有结果汇总

---

``<span lang="EN-US">`7. create_submission(row_ids, predictions, species_ids,

cfg)```<span>`：生成提交`<span lang="EN-US">` DataFrame<o:p></o:p>`

#### 功能：

将预测结果整理为符合比赛要求的提交格式。

#### 步骤说明：

构建字典结构

补全缺失的物种列（如训练集未覆盖）

确保列顺序与样本提交一致

---

`<span lang="EN-US"><o:p> </o:p></span>`

`<span lang="EN-US"><o:p> </o:p></span>`

`<span lang="EN-US"><o:p> </o:p></span>`

`<span lang="EN-US">8. smooth_submission(submission_path)</span>``<span>：后处理平滑<span lang="EN-US"><o:p></o:p></span></span>`

#### 功能：

对提交结果进行时间上的平滑处理，提升预测的时间一致性。

#### 方法说明：

对每个 soundscape 分组

对预测值进行加权平均（当前帧 × 0.6 + 上下帧 × 0.2）

特别处理首尾帧

模型二和模型一主要差别

backbone不一样

Infer时模型二使用max，模型一使用mean，请自行查看代码对比

训练来源：

[https://github.com/LIHANG-HONG/birdclef2023-2nd-place-solution](https://github.com/LIHANG-HONG/birdclef2023-2nd-place-solution)

（非重点，不介绍）

**4.2 ****优化代码解析（最终方案）**

优化代码：[https://www.kaggle.com/code/hjhhhk/bird25-weightedblend-nfnet-seresnext-cd27d6?scriptVersionId=243872162](https://www.kaggle.com/code/hjhhhk/bird25-weightedblend-nfnet-seresnext-cd27d6?scriptVersionId=243872162)

优化思路：

模型推理的主要权重为模型二 0.75的权重占比。

使用集成的思路重新训练另一个模型权重，对模型二进行集成推理。

训练的代码文件：AA842_testn.ipynb

训练依赖tar文件两个

训练过程解析：

`<b><span lang="EN-US">1.</span></b>``<b><span>配置准备与初始化<span lang="EN-US"><o:p></o:p></span></span></b>`

加载预设的训练配置（如模型参数、优化器设置、数据路径等）。

设置随机种子以确保实验可重复。

读取分类标签信息，并根据配置选择训练设备（如 GPU）。

`<b><span lang="EN-US">2.</span></b>``<b><span>数据预处理与加载<span lang="EN-US"><o:p></o:p></span></span></b>`

根据设定的配置对训练和验证数据进行预处理，包括音频特征提取（如 Mel 频谱图）。

构建训练和验证数据的数据加载器（Dataloader），支持批量读取和数据增强。

`<b><span lang="EN-US">3.</span></b>``<b><span>模型构与优化器设置<span lang="EN-US"><o:p></o:p></span></span></b>`

加载指定的神经网络模型（如 seresnext26t_32x4d）。

初始化优化器（如AdamW）、学习率调度器（如 CosineAnnealingLR）和损失函数（结合 BCE 和 Focal Loss）。

`<b><span lang="EN-US">4.</span></b>``<b><span>模型训练<span lang="EN-US"><o:p></o:p></span></span></b>`

对每个训练轮次（epoch）：

模型前向传播，计算输出和损失。

反向传播更新模型参数。

跟踪训练损失并显示进度。

使用学习率调度器调整学习率。

保存每一轮训练后的模型权重。

`<b><span lang="EN-US">5.</span></b>``<b><span>资源清理</span></b>`

训练完成后释放内存和 GPU 显存资源，避免占用过多系统资源。

整个训练流程围绕音频分类任务展开，使用了图像化音频特征（如频谱图）作为输入，采用深度卷积神经网络进行端到端训练。
